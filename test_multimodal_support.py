"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€åŠŸèƒ½

æ­¤è„šæœ¬ç”¨äºæµ‹è¯• Together AIã€Groq æˆ–å…¶ä»–æ¨¡å‹æ˜¯å¦æ”¯æŒ MERI é¡¹ç›®éœ€è¦çš„å¤šæ¨¡æ€åŠŸèƒ½ï¼š
1. æ–‡æœ¬ + å›¾ç‰‡ï¼ˆbase64 æ ¼å¼ï¼‰
2. å‡½æ•°è°ƒç”¨ï¼ˆtoolsï¼‰
3. JSON æ ¼å¼è¾“å‡ºï¼ˆresponse_formatï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
1. åœ¨ .env æ–‡ä»¶ä¸­é…ç½®å¯¹åº”çš„ API Key
2. ä¿®æ”¹ä¸‹é¢çš„ model_name å˜é‡
3. è¿è¡Œï¼špython test_multimodal_support.py
"""

from litellm import completion
import os
from dotenv import load_dotenv
import base64

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def create_test_image_base64():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾ç‰‡ï¼ˆ1x1 çº¢è‰²åƒç´ ï¼‰çš„ base64 ç¼–ç """
    # åˆ›å»ºä¸€ä¸ªæœ€å°çš„ PNG å›¾ç‰‡ï¼ˆ1x1 çº¢è‰²åƒç´ ï¼‰
    # è¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ PNG å›¾ç‰‡çš„ base64 ç¼–ç 
    png_base64 = "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg=="
    return f"data:image/png;base64,{png_base64}"

def test_multimodal_support(model_name):
    """æµ‹è¯•æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€åŠŸèƒ½"""
    
    print("=" * 60)
    print(f"æµ‹è¯•æ¨¡å‹ï¼š{model_name}")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡ï¼‰
    test_image = create_test_image_base64()
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹ã€‚å¦‚æœèƒ½çœ‹åˆ°å›¾ç‰‡ï¼Œè¯·å›ç­”'æˆ‘çœ‹åˆ°äº†å›¾ç‰‡'ï¼Œå¦åˆ™è¯·è¯´æ˜é”™è¯¯åŸå› ã€‚"},
                {"type": "image_url", "image_url": {"url": test_image}}
            ]
        }
    ]
    
    print("\nğŸ“ æµ‹è¯• 1ï¼šå¤šæ¨¡æ€æ”¯æŒï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰")
    print("-" * 60)
    
    try:
        response = completion(
            model=model_name,
            messages=messages,
            max_tokens=100,
            temperature=0.0,
        )
        
        print("âœ… API è°ƒç”¨æˆåŠŸï¼")
        print(f"ğŸ“„ å“åº”å†…å®¹ï¼š{response.choices[0].message.content}")
        
        # æ£€æŸ¥å“åº”æ˜¯å¦è¡¨æ˜çœ‹åˆ°äº†å›¾ç‰‡
        response_text = response.choices[0].message.content.lower()
        if "å›¾ç‰‡" in response_text or "image" in response_text or "çœ‹åˆ°" in response_text:
            print("âœ… å¤šæ¨¡æ€æ”¯æŒï¼šæ¨¡å‹èƒ½å¤Ÿå¤„ç†å›¾ç‰‡")
            multimodal_supported = True
        else:
            print("âš ï¸  å¤šæ¨¡æ€æ”¯æŒï¼šä¸ç¡®å®šï¼ˆè¯·æŸ¥çœ‹å“åº”å†…å®¹ï¼‰")
            multimodal_supported = None
            
    except Exception as e:
        print(f"âŒ API è°ƒç”¨å¤±è´¥ï¼š{e}")
        print("âŒ å¤šæ¨¡æ€æ”¯æŒï¼šä¸æ”¯æŒæˆ–é…ç½®é”™è¯¯")
        multimodal_supported = False
        return multimodal_supported
    
    print("\nğŸ“ æµ‹è¯• 2ï¼šå‡½æ•°è°ƒç”¨æ”¯æŒï¼ˆtoolsï¼‰")
    print("-" * 60)
    
    # åˆ›å»ºç®€å•çš„å‡½æ•°è°ƒç”¨æµ‹è¯•
    tools = [{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "åŸå¸‚åç§°"}
                },
                "required": ["location"]
            }
        }
    }]
    
    text_messages = [
        {"role": "user", "content": "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}
    ]
    
    try:
        response = completion(
            model=model_name,
            messages=text_messages,
            tools=tools,
            tool_choice="auto",
            max_tokens=100,
            temperature=0.0,
        )
        
        print("âœ… API è°ƒç”¨æˆåŠŸï¼")
        if hasattr(response.choices[0].message, 'tool_calls') and response.choices[0].message.tool_calls:
            print("âœ… å‡½æ•°è°ƒç”¨æ”¯æŒï¼šæ¨¡å‹æ”¯æŒ tools å‚æ•°")
            tools_supported = True
        else:
            print("âš ï¸  å‡½æ•°è°ƒç”¨æ”¯æŒï¼šæ¨¡å‹å¯èƒ½ä¸æ”¯æŒæˆ–æœªè§¦å‘å‡½æ•°è°ƒç”¨")
            tools_supported = None
            
    except Exception as e:
        print(f"âŒ API è°ƒç”¨å¤±è´¥ï¼š{e}")
        print("âŒ å‡½æ•°è°ƒç”¨æ”¯æŒï¼šä¸æ”¯æŒæˆ–é…ç½®é”™è¯¯")
        tools_supported = False
    
    print("\nğŸ“ æµ‹è¯• 3ï¼šJSON æ ¼å¼è¾“å‡ºæ”¯æŒï¼ˆresponse_formatï¼‰")
    print("-" * 60)
    
    json_messages = [
        {"role": "user", "content": "è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼š{\"name\": \"æµ‹è¯•\", \"value\": 123}"}
    ]
    
    try:
        response = completion(
            model=model_name,
            messages=json_messages,
            response_format={"type": "json_object"},
            max_tokens=100,
            temperature=0.0,
        )
        
        print("âœ… API è°ƒç”¨æˆåŠŸï¼")
        print(f"ğŸ“„ å“åº”å†…å®¹ï¼š{response.choices[0].message.content}")
        
        # å°è¯•è§£æ JSON
        import json
        try:
            json.loads(response.choices[0].message.content)
            print("âœ… JSON æ ¼å¼è¾“å‡ºæ”¯æŒï¼šæ¨¡å‹æ”¯æŒ response_format")
            json_supported = True
        except:
            print("âš ï¸  JSON æ ¼å¼è¾“å‡ºæ”¯æŒï¼šå“åº”ä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼ˆå¯èƒ½é€šè¿‡ prompt å®ç°ï¼‰")
            json_supported = None
            
    except Exception as e:
        error_msg = str(e).lower()
        if "response_format" in error_msg or "json" in error_msg:
            print(f"âŒ API è°ƒç”¨å¤±è´¥ï¼š{e}")
            print("âŒ JSON æ ¼å¼è¾“å‡ºæ”¯æŒï¼šä¸æ”¯æŒ response_format å‚æ•°")
            print("ğŸ’¡ æç¤ºï¼šå¯ä»¥é€šè¿‡åœ¨ prompt ä¸­è¦æ±‚ JSON æ ¼å¼æ¥æ›¿ä»£")
            json_supported = False
        else:
            print(f"âŒ API è°ƒç”¨å¤±è´¥ï¼š{e}")
            json_supported = False
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"å¤šæ¨¡æ€æ”¯æŒï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰ï¼š{'âœ… æ”¯æŒ' if multimodal_supported else 'âŒ ä¸æ”¯æŒ' if multimodal_supported is False else 'âš ï¸  ä¸ç¡®å®š'}")
    print(f"å‡½æ•°è°ƒç”¨æ”¯æŒï¼ˆtoolsï¼‰ï¼š{'âœ… æ”¯æŒ' if tools_supported else 'âŒ ä¸æ”¯æŒ' if tools_supported is False else 'âš ï¸  ä¸ç¡®å®š'}")
    print(f"JSON æ ¼å¼è¾“å‡ºæ”¯æŒï¼š{'âœ… æ”¯æŒ' if json_supported else 'âŒ ä¸æ”¯æŒ' if json_supported is False else 'âš ï¸  ä¸ç¡®å®š'}")
    
    print("\nğŸ’¡ å»ºè®®ï¼š")
    if multimodal_supported:
        print("  âœ… æ¨¡å‹æ”¯æŒå¤šæ¨¡æ€ï¼Œå¯ä»¥ç”¨äº MERI é¡¹ç›®")
    else:
        print("  âŒ æ¨¡å‹ä¸æ”¯æŒå¤šæ¨¡æ€ï¼Œæ— æ³•ç”¨äº MERI é¡¹ç›®ï¼ˆéœ€è¦å¤„ç† PDF ä¸­çš„å›¾ç‰‡ï¼‰")
        print("  ğŸ’¡ å»ºè®®å°è¯•å…¶ä»–æ¨¡å‹ï¼Œå¦‚ Hugging Face çš„å¤šæ¨¡æ€æ¨¡å‹")
    
    if not tools_supported:
        print("  âš ï¸  æ¨¡å‹å¯èƒ½ä¸æ”¯æŒå‡½æ•°è°ƒç”¨ï¼ŒMERI é¡¹ç›®å¯èƒ½éœ€è¦æ­¤åŠŸèƒ½")
    
    if not json_supported:
        print("  âš ï¸  æ¨¡å‹ä¸æ”¯æŒ response_formatï¼Œä½†å¯ä»¥é€šè¿‡ prompt è¦æ±‚ JSON æ ¼å¼")
    
    return {
        "multimodal": multimodal_supported,
        "tools": tools_supported,
        "json": json_supported
    }

if __name__ == "__main__":
    # åœ¨è¿™é‡Œä¿®æ”¹è¦æµ‹è¯•çš„æ¨¡å‹åç§°
    # ç¤ºä¾‹ï¼š
    # model_name = "together_ai/llava-1.5-7b"
    # model_name = "groq/llama-3.2-11b-vision-preview"
    # model_name = "huggingface/Qwen/Qwen2-VL-2B-Instruct"
    # model_name = "gemini/gemini-1.5-flash"
    
    print("è¯·ä¿®æ”¹è„šæœ¬ä¸­çš„ model_name å˜é‡æ¥æµ‹è¯•ä¸åŒçš„æ¨¡å‹")
    print("\nå¯æµ‹è¯•çš„æ¨¡å‹ç¤ºä¾‹ï¼š")
    print("  - together_ai/llava-1.5-7b")
    print("  - groq/llama-3.2-11b-vision-preview")
    print("  - huggingface/Qwen/Qwen2-VL-2B-Instruct")
    print("  - gemini/gemini-1.5-flash")
    print()
    
    # å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šå¹¶ä¿®æ”¹æ¨¡å‹åç§°æ¥æµ‹è¯•
    # model_name = "together_ai/llava-1.5-7b"
    # test_multimodal_support(model_name)




